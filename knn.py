# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QA-tHm5WHExL2_MckIvJDNRofPISAppx

### üîç **KNN (K-Nearest Neighbors)**

**1. What is KNN and how does it work?**
KNN is a non-parametric, instance-based algorithm that classifies or predicts a sample based on the majority label or average value of its K nearest neighbors in feature space.

**2. Difference between KNN Classification and Regression:**

* *Classification:* Predicts the most common class among K neighbors.
* *Regression:* Predicts the average of the target values of K neighbors.

**3. Role of distance metric in KNN:**
Distance metrics (e.g., Euclidean, Manhattan) determine which neighbors are "nearest" and greatly impact model accuracy.

**4. Curse of Dimensionality in KNN:**
In high-dimensional spaces, all points tend to become equidistant, making it hard for KNN to find true neighbors and degrading performance.

**5. Choosing the best value of K:**
Use cross-validation to evaluate different K values. Too low (e.g., K=1) can overfit; too high may underfit.

**6. KD Tree and Ball Tree:**
Data structures that speed up nearest neighbor searches:

* *KD Tree:* Efficient in low dimensions.
* *Ball Tree:* Handles higher dimensions better.

**7. When to use KD Tree vs. Ball Tree:**

* Use KD Tree if dimensions < 20.
* Use Ball Tree for higher-dimensional or unevenly distributed data.

**8. Disadvantages of KNN:**

* Slow at prediction time.
* Sensitive to feature scaling and irrelevant features.
* Performs poorly in high-dimensional data.

**9. Feature scaling effect on KNN:**
KNN is distance-based, so features must be scaled (e.g., using StandardScaler or MinMaxScaler) to avoid bias from large-magnitude features.

**10. How does KNN handle missing values?**
KNN doesn‚Äôt handle missing values directly. Preprocessing steps like imputation are necessary.

---

### üìä **PCA (Principal Component Analysis)**

**11. What is PCA?**
PCA is a dimensionality reduction technique that transforms data into a new coordinate system based on variance, reducing redundancy.

**12. How does PCA work?**

* Standardize data.
* Compute covariance matrix.
* Calculate eigenvectors/eigenvalues.
* Project data onto top components.

**13. Geometric intuition of PCA:**
PCA rotates the coordinate system so that the axes (principal components) align with the directions of maximum variance in the data.

**14. Feature Selection vs. Feature Extraction:**

* *Selection:* Chooses existing features.
* *Extraction:* Creates new features (e.g., PCA transforms data into components).

**15. Eigenvalues and Eigenvectors in PCA:**

* *Eigenvectors:* Define new feature space directions.
* *Eigenvalues:* Measure variance captured by each eigenvector.

**16. Deciding number of components to keep in PCA:**
Choose enough components to retain a high percentage (e.g., 95%) of explained variance.

**17. Can PCA be used for classification?**
Yes, after reducing dimensions with PCA, classifiers like KNN or SVM can be applied more effectively.

**18. Limitations of PCA:**

* Assumes linear relationships.
* Sensitive to scaling.
* Harder to interpret components.

**19. How do KNN and PCA complement each other?**
PCA reduces noise and dimensionality, improving KNN‚Äôs efficiency and accuracy, especially in high-dimensional data.

**20. PCA vs. LDA:**

* *PCA:* Unsupervised, maximizes variance.
* *LDA:* Supervised, maximizes class separation.

# Import libraries (do this only once)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris, make_regression, make_classification
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, classification_report
from sklearn.decomposition import PCA
from sklearn.impute import KNNImputer
from sklearn.pipeline import Pipeline

# 1. Train a KNN Classifier on the Iris dataset
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
print("Iris KNN Accuracy:", accuracy_score(y_test, knn.predict(X_test)))

# 2. Train a KNN Regressor on synthetic dataset
X_reg, y_reg = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)
knn_reg = KNeighborsRegressor()
knn_reg.fit(X_train_r, y_train_r)
print("KNN Regressor MSE:", mean_squared_error(y_test_r, knn_reg.predict(X_test_r)))

# 3. KNN Classifier using different distance metrics
for metric in ['euclidean', 'manhattan']:
    model = KNeighborsClassifier(metric=metric)
    model.fit(X_train, y_train)
    acc = accuracy_score(y_test, model.predict(X_test))
    print(f"{metric.title()} Distance Accuracy: {acc:.2f}")

# 4. Visualize decision boundaries for different K
from matplotlib.colors import ListedColormap
X_vis = iris.data[:, :2]
y_vis = iris.target
h = .02

for k in [1, 5, 10]:
    clf = KNeighborsClassifier(n_neighbors=k)
    clf.fit(X_vis, y_vis)
    x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1
    y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']))
    plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_vis, edgecolor='k', cmap=ListedColormap(['#FF0000', '#00FF00', '#0000FF']))
    plt.title(f"K = {k} Decision Boundary")
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

# 5. Compare with and without Feature Scaling
unscaled_model = KNeighborsClassifier()
unscaled_model.fit(X_train, y_train)
unscaled_acc = accuracy_score(y_test, unscaled_model.predict(X_test))

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
scaled_model = KNeighborsClassifier()
scaled_model.fit(X_train_scaled, y_train)
scaled_acc = accuracy_score(y_test, scaled_model.predict(X_test_scaled))
print(f"Unscaled Accuracy: {unscaled_acc:.2f}, Scaled Accuracy: {scaled_acc:.2f}")

# 6. Train PCA on synthetic data
X_syn, _ = make_classification(n_samples=100, n_features=5, random_state=42)
pca = PCA()
pca.fit(X_syn)
print("Explained Variance Ratio:", pca.explained_variance_ratio_)

# 7. Apply PCA before training KNN Classifier
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=2)),
    ('knn', KNeighborsClassifier())
])
pipeline.fit(X_train, y_train)
print("PCA + KNN Accuracy:", pipeline.score(X_test, y_test))

# 8. Hyperparameter tuning using GridSearchCV
params = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}
gs = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=3)
gs.fit(X_train, y_train)
print("Best Parameters:", gs.best_params_)
print("Best Score:", gs.best_score_)

# 9. Misclassified samples
predictions = knn.predict(X_test)
misclassified = (predictions != y_test).sum()
print("Misclassified Samples:", misclassified)

# 10. Cumulative explained variance
pca = PCA().fit(X_syn)
cum_var = np.cumsum(pca.explained_variance_ratio_)
plt.figure()
plt.plot(range(1, len(cum_var)+1), cum_var, marker='o')
plt.title("Cumulative Explained Variance")
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Variance")
plt.grid()
plt.show()
"""