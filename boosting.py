# -*- coding: utf-8 -*-
"""Boosting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l64nZ0VTJ7hXdocUWObmefeqCOcJISMr

**What is Boosting in Machine Learning?**
Boosting is an ensemble technique that combines multiple weak learners sequentially to create a strong learner by focusing on mistakes of previous models.

---

**How does Boosting differ from Bagging?**
Boosting builds models sequentially, each correcting errors of the previous, while Bagging builds models independently in parallel and aggregates their predictions.

---

**What is the key idea behind AdaBoost?**
AdaBoost adjusts weights on training samples to focus more on misclassified points, combining weak learners into a strong classifier by weighted voting.

---

**Explain the working of AdaBoost with an example**
AdaBoost trains a sequence of weak classifiers; after each iteration, it increases the weight of misclassified samples, forcing the next classifier to focus on harder cases. Final prediction is a weighted sum of all classifiers.

---

**What is Gradient Boosting, and how is it different from AdaBoost?**
Gradient Boosting builds models sequentially by fitting new models to the residual errors (gradients) of previous models, optimizing a differentiable loss function, while AdaBoost reweights samples explicitly.

---

**What is the loss function in Gradient Boosting?**
It depends on the problem; common ones are squared error loss for regression and exponential/logistic loss for classification.

---

**How does XGBoost improve over traditional Gradient Boosting?**
XGBoost includes regularization, parallel processing, tree pruning, handling missing values, and optimized hardware utilization, making it faster and more accurate.

---

**What is the difference between XGBoost and CatBoost?**
CatBoost is optimized for categorical features with special encoding, reducing overfitting and improving speed, while XGBoost requires explicit categorical encoding.

---

**What are some real-world applications of Boosting techniques?**
Used in fraud detection, customer churn prediction, credit scoring, recommendation systems, and bioinformatics.

---

**How does regularization help in XGBoost?**
Regularization (L1/L2) controls model complexity to prevent overfitting and improve generalization.

---

**What are some hyperparameters to tune in Gradient Boosting models?**
Learning rate, number of estimators (trees), max depth, min samples split, subsample ratio, and regularization terms.

---

**What is the concept of Feature Importance in Boosting?**
It measures how much each feature contributes to reducing the loss across all trees, helping interpret the model.

---

**Why is CatBoost efficient for categorical data?**
CatBoost uses ordered boosting and target-based statistics encoding that handle categorical variables internally without preprocessing, reducing bias and overfitting.

```python
# Import libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer, load_boston, load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from xgboost import XGBClassifier, XGBRegressor
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score, mean_squared_error, f1_score, confusion_matrix, roc_curve, auc, log_loss
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Train AdaBoost Classifier and print accuracy
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
ada_clf = AdaBoostClassifier(random_state=42).fit(X_train, y_train)
print("AdaBoost Classifier Accuracy:", accuracy_score(y_test, ada_clf.predict(X_test)))

# 2. Train AdaBoost Regressor and evaluate MAE
X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
ada_reg = AdaBoostRegressor(random_state=42).fit(X_train, y_train)
y_pred = ada_reg.predict(X_test)
print("AdaBoost Regressor MAE:", mean_absolute_error(y_test, y_pred))

# 3. Gradient Boosting Classifier on Breast Cancer and print feature importance
data = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)
gb_clf = GradientBoostingClassifier(random_state=42).fit(X_train, y_train)
print("Feature Importances:", gb_clf.feature_importances_)

# 4. Gradient Boosting Regressor and R2 Score
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
gb_reg = GradientBoostingRegressor(random_state=42).fit(X_train, y_train)
print("Gradient Boosting Regressor R2:", r2_score(y_test, gb_reg.predict(X_test)))

# 5. Train XGBoost Classifier and compare accuracy with Gradient Boosting
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss').fit(X_train, y_train)
print("XGBoost Classifier Accuracy:", accuracy_score(y_test, xgb_clf.predict(X_test)))
print("Gradient Boosting Accuracy:", accuracy_score(y_test, gb_clf.predict(X_test)))

# 6. Train CatBoost Classifier and evaluate F1-score
cat_clf = CatBoostClassifier(verbose=0, random_state=42).fit(X_train, y_train)
print("CatBoost Classifier F1-score:", f1_score(y_test, cat_clf.predict(X_test), average='weighted'))

# 7. Train XGBoost Regressor and evaluate MSE
xgb_reg = XGBRegressor(random_state=42).fit(X_train, y_train)
print("XGBoost Regressor MSE:", mean_squared_error(y_test, xgb_reg.predict(X_test)))

# 8. AdaBoost Classifier and visualize feature importance
plt.bar(range(len(ada_clf.feature_importances_)), ada_clf.feature_importances_)
plt.title("AdaBoost Feature Importances")
plt.show()

# 9. Gradient Boosting Regressor learning curves plot
from sklearn.model_selection import learning_curve
train_sizes, train_scores, val_scores = learning_curve(GradientBoostingRegressor(random_state=42), X_train, y_train, cv=5)
plt.plot(train_sizes, np.mean(train_scores, axis=1), label="Training score")
plt.plot(train_sizes, np.mean(val_scores, axis=1), label="Validation score")
plt.legend()
plt.title("Gradient Boosting Learning Curve")
plt.show()

# 10. XGBoost Classifier feature importance visualization
xgb.plot_importance(xgb_clf)
plt.show()

# 11. CatBoost Classifier confusion matrix
y_pred = cat_clf.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d")
plt.title("CatBoost Confusion Matrix")
plt.show()

# 12. AdaBoost Classifier with different n_estimators and compare accuracy
estimators = [10, 50, 100]
for n in estimators:
    model = AdaBoostClassifier(n_estimators=n, random_state=42).fit(X_train, y_train)
    print(f"n_estimators={n} Accuracy: {accuracy_score(y_test, model.predict(X_test))}")

# 13. Gradient Boosting Classifier ROC Curve
from sklearn.preprocessing import label_binarize
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
gb_probs = gb_clf.predict_proba(X_test)
fpr, tpr, _ = roc_curve(y_test_bin[:, 0], gb_probs[:, 0])
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Gradient Boosting")
plt.legend()
plt.show()

# 14. XGBoost Regressor GridSearchCV tuning learning rate
params = {'learning_rate': [0.01, 0.1, 0.2]}
grid = GridSearchCV(XGBRegressor(random_state=42), param_grid=params, cv=3)
grid.fit(X_train, y_train)
print("Best learning rate:", grid.best_params_)

# 15. CatBoost Classifier on imbalanced dataset (simulate imbalance)
from sklearn.utils import resample
X_imb, y_imb = resample(X_train[y_train==0], y_train[y_train==0], n_samples=10), np.zeros(10)
X_bal, y_bal = X_train[y_train==1], y_train[y_train==1]
X_new = np.vstack((X_imb, X_bal))
y_new = np.concatenate((y_imb, y_bal))
cat_clf_imbalanced = CatBoostClassifier(verbose=0, random_state=42).fit(X_new, y_new)
print("CatBoost F1-score on imbalanced data:", f1_score(y_test, cat_clf_imbalanced.predict(X_test)))

# 16. AdaBoost Classifier effect of different learning rates
for lr in [0.01, 0.1, 1]:
    model = AdaBoostClassifier(learning_rate=lr, random_state=42).fit(X_train, y_train)
    print(f"Learning rate={lr} Accuracy: {accuracy_score(y_test, model.predict(X_test))}")

# 17. XGBoost Classifier multi-class classification and log-loss
xgb_multi = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42).fit(X_train, y_train)
y_pred_proba = xgb_multi.predict_proba(X_test)
print("XGBoost multi-class log-loss:", log_loss(y_test, y_pred_proba))
```
"""