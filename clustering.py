# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l64nZ0VTJ7hXdocUWObmefeqCOcJISMr

### What is unsupervised learning in machine learning?

Unsupervised learning is a type of machine learning where the model learns patterns from unlabeled data, without predefined outputs. It identifies inherent structures, such as clusters or associations, within the data.

---

### How does K-Means clustering algorithm work?

K-Means partitions data into K clusters by iteratively:

1. Initializing K centroids
2. Assigning points to the nearest centroid
3. Recalculating centroids as the mean of assigned points
4. Repeating until convergence (no change in clusters)

---

### Explain the concept of a dendrogram in hierarchical clustering

A dendrogram is a tree-like diagram that shows the arrangement of clusters formed at each step of hierarchical clustering. It visually represents the merging or splitting of clusters and helps decide the number of clusters by cutting the tree at a certain height.

---

### Main difference between K-Means and Hierarchical Clustering

* K-Means requires specifying the number of clusters (K) upfront and partitions data flatly.
* Hierarchical clustering builds a tree of clusters without needing a preset number, offering nested groupings.

---

### Advantages of DBSCAN over K-Means

* DBSCAN can find clusters of arbitrary shape, while K-Means assumes spherical clusters.
* DBSCAN identifies noise/outliers explicitly.
* DBSCAN doesn’t require specifying the number of clusters in advance.

---

### When would you use Silhouette Score in clustering?

Silhouette Score evaluates how well clusters are separated and how cohesive they are, helping to assess cluster quality and choose the optimal number of clusters.

---

### Limitations of Hierarchical Clustering

* Computationally expensive for large datasets (scales poorly).
* Once merged/split, clusters cannot be undone (no reassignments).
* Sensitive to noise and outliers.

---

### Why is feature scaling important in clustering algorithms like K-Means?

Because K-Means uses Euclidean distance, features on larger scales dominate distance calculations, biasing clustering results. Scaling ensures all features contribute equally.

---

### How does DBSCAN identify noise points?

Points that do not have enough neighboring points within the radius (epsilon) threshold to form a dense region are labeled as noise.

---

### Define inertia in the context of K-Means

Inertia is the sum of squared distances between each point and its cluster centroid; it measures cluster compactness. Lower inertia means tighter clusters.

---

### What is the elbow method in K-Means clustering?

A technique to select the optimal number of clusters by plotting inertia vs. number of clusters and looking for a “knee” or elbow point where the rate of decrease sharply changes.

---

### Describe the concept of "density" in DBSCAN

Density refers to the number of points within a neighborhood (radius epsilon). Clusters are formed from regions with high point density separated by low-density areas.

---

### Can hierarchical clustering be used on categorical data?

Yes, if an appropriate distance metric for categorical data (e.g., Hamming distance) is used, hierarchical clustering can handle categorical attributes.

---

### What does a negative Silhouette Score indicate?

Points are likely assigned to the wrong cluster and are closer to neighboring clusters than their own, indicating poor clustering.

---

### Explain the term "linkage criteria" in hierarchical clustering

Linkage criteria define how distances between clusters are calculated during merging, e.g., single linkage (minimum distance), complete linkage (maximum), average linkage, or ward’s method.

---

### Why might K-Means perform poorly on data with varying cluster sizes or densities?

Because K-Means assumes clusters are spherical and roughly equal in size/density, it struggles with irregular clusters, often merging small clusters or splitting large ones incorrectly.

---

### What are the core parameters in DBSCAN, and how do they influence clustering?

* **Epsilon (ε):** Radius to search neighbors.
* **MinPts:** Minimum points to form a dense region.
  Together, they control cluster size, shape, and noise identification.

---

### How does K-Means++ improve upon standard K-Means initialization?

K-Means++ selects initial centroids more carefully by spreading them out, improving convergence speed and clustering quality compared to random initialization.

---

### What is agglomerative clustering?

A bottom-up hierarchical method where each point starts as its own cluster, then pairs of clusters are merged stepwise based on linkage criteria until one cluster remains or a stopping condition is met.

---

### Why is Silhouette Score better than inertia for model evaluation?

Inertia only measures compactness, ignoring cluster separation, while Silhouette Score considers both cohesion and separation, giving a fuller picture of clustering quality.

---

```python
# Libraries
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_blobs, make_moons, make_circles, load_iris, load_wine, load_breast_cancer, load_digits
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage
```

---

> **Generate synthetic data with 4 centers using make\_blobs and apply K-Means clustering. Visualize scatter plot**

```python
X, _ = make_blobs(n_samples=300, centers=4, random_state=42)
kmeans = KMeans(n_clusters=4).fit(X)
plt.scatter(X[:,0], X[:,1], c=kmeans.labels_)
plt.title('K-Means with 4 centers')
plt.show()
```

---

> **Load Iris dataset and use Agglomerative Clustering for 3 clusters. Display first 10 labels**

```python
iris = load_iris()
agg = AgglomerativeClustering(n_clusters=3).fit(iris.data)
print(agg.labels_[:10])
```

---

> **Generate make\_moons data and apply DBSCAN. Highlight outliers**

```python
X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)
db = DBSCAN(eps=0.2, min_samples=5).fit(X)
plt.scatter(X[:,0], X[:,1], c=db.labels_, cmap='viridis')
outliers = X[db.labels_ == -1]
plt.scatter(outliers[:,0], outliers[:,1], color='red', label='Outliers')
plt.legend()
plt.show()
```

---

> **Load Wine dataset, standardize, apply K-Means, print cluster sizes**

```python
wine = load_wine()
scaler = StandardScaler()
X_scaled = scaler.fit_transform(wine.data)
kmeans = KMeans(n_clusters=3).fit(X_scaled)
unique, counts = np.unique(kmeans.labels_, return_counts=True)
print(dict(zip(unique, counts)))
```

---

> **Use make\_circles, cluster with DBSCAN, plot results**

```python
X, _ = make_circles(noise=0.05, factor=0.5, random_state=42)
db = DBSCAN(eps=0.1, min_samples=5).fit(X)
plt.scatter(X[:,0], X[:,1], c=db.labels_, cmap='coolwarm')
plt.show()
```

---

> **Load Breast Cancer dataset, apply MinMaxScaler and KMeans(2), output centroids**

```python
bc = load_breast_cancer()
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(bc.data)
kmeans = KMeans(n_clusters=2).fit(X_scaled)
print(kmeans.cluster_centers_)
```

---

> **make\_blobs with varying std deviations and cluster with DBSCAN**

```python
X, _ = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)
db = DBSCAN(eps=0.8, min_samples=5).fit(X)
plt.scatter(X[:,0], X[:,1], c=db.labels_)
plt.show()
```

---

> **Load Digits, reduce to 2D with PCA, visualize KMeans clusters**

```python
digits = load_digits()
pca = PCA(n_components=2)
X_pca = pca.fit_transform(digits.data)
kmeans = KMeans(n_clusters=10).fit(X_pca)
plt.scatter(X_pca[:,0], X_pca[:,1], c=kmeans.labels_, cmap='tab10')
plt.show()
```

---

> **make\_blobs and silhouette scores for k=2 to 5, plot bar chart**

```python
X, _ = make_blobs(n_samples=300, centers=4, random_state=42)
scores = []
ks = range(2,6)
for k in ks:
    kmeans = KMeans(n_clusters=k).fit(X)
    scores.append(silhouette_score(X, kmeans.labels_))
plt.bar(ks, scores)
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.show()
```

---

> **Load Iris dataset, hierarchical clustering, plot dendrogram with average linkage**

```python
iris = load_iris()
Z = linkage(iris.data, method='average')
dendrogram(Z)
plt.title('Dendrogram - Average linkage')
plt.show()
```

---

> **make\_blobs with overlapping clusters, K-Means, visualize with decision boundaries**

```python
from matplotlib.colors import ListedColormap
X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)
kmeans = KMeans(n_clusters=3).fit(X)
h = .02
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.3, cmap='Pastel1')
plt.scatter(X[:,0], X[:,1], c=kmeans.labels_)
plt.show()
```

---

> **Load Digits dataset, reduce with t-SNE, apply DBSCAN, visualize**

```python
digits = load_digits()
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(digits.data)
db = DBSCAN(eps=3, min_samples=5).fit(X_tsne)
plt.scatter(X_tsne[:,0], X_tsne[:,1], c=db.labels_, cmap='tab10')
plt.show()
```

---

> **make\_blobs, Agglomerative Clustering with complete linkage, plot**

```python
X, _ = make_blobs(n_samples=300, centers=3, random_state=42)
agg = AgglomerativeClustering(n_clusters=3, linkage='complete').fit(X)
plt.scatter(X[:,0], X[:,1], c=agg.labels_)
plt.show()
```

---

> **Load Breast Cancer, compare inertia for k=2 to 6, line plot**

```python
bc = load_breast_cancer()
inertias = []
ks = range(2,7)
for k in ks:
    kmeans = KMeans(n_clusters=k).fit(bc.data)
    inertias.append(kmeans.inertia_)
plt.plot(ks, inertias, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()
```

---

> **Generate concentric circles with make\_circles, cluster with Agglomerative Clustering (single linkage)**

```python
X, _ = make_circles(noise=0.05, factor=0.3, random_state=42)
agg = AgglomerativeClustering(n_clusters=2, linkage='single').fit(X)
plt.scatter(X[:,0], X[:,1], c=agg.labels_)
plt.show()
```

---

> **Use Wine dataset, apply DBSCAN after scaling, count clusters excluding noise**

```python
wine = load_wine()
scaler = StandardScaler()
X_scaled = scaler.fit_transform(wine.data)
db = DBSCAN(eps=1.5, min_samples=5).fit(X_scaled)
n_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)
print("Clusters found:", n_clusters)
```

---

> **make\_blobs, apply KMeans, plot cluster centers on data**

```python
X, _ = make_blobs(n_samples=300, centers=3, random_state=42)
kmeans = KMeans(n_clusters=3).fit(X)
plt.scatter(X[:,0], X[:,1], c=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=150, c='red', marker='X')
plt.show()
```

---

> **make\_blobs with 5 centers, apply KMeans, evaluate with silhouette\_score**

```python
X, _ = make_blobs(n_samples=500, centers=5, random_state=42)
kmeans = KMeans(n_clusters=5).fit(X)
score = silhouette_score(X, kmeans.labels_)
print("Silhouette Score:", score)
```

---

> **Load Breast Cancer, reduce with PCA, Agglomerative Clustering, visualize 2D**

```python
bc = load_breast_cancer()
pca = PCA(n_components=2)
X_pca = pca.fit_transform(bc.data)
agg = AgglomerativeClustering(n_clusters=2).fit(X_pca)
plt.scatter(X_pca[:,0], X_pca[:,1], c=agg.labels_)
plt.show()
```

---

> **Noisy circular data make\_circles, KMeans and DBSCAN clustering side-by-side**

```python
X, _ = make_circles(noise=0.05, factor=0.5, random_state=42)
fig, axes = plt.subplots(1, 2, figsize=(12,5))
kmeans = KMeans(n_clusters=2).fit(X)
axes[0].scatter(X[:,0], X[:,1], c=kmeans.labels_)
axes[0].set_title('KMeans')
db = DBSCAN(eps=0.1, min_samples=5).fit(X)
axes[1].scatter(X[:,0], X[:,1], c=db.labels_)
axes[1].set_title('DBSCAN')
plt.show()
```

---

> **Load Iris, plot Silhouette Coefficient for each sample after KMeans**

```python
iris = load_iris()
kmeans = KMeans(n_clusters=3).fit(iris.data)
sil_vals = silhouette_score(iris.data, kmeans.labels_, sample_size=len(iris.data))
print("Average Silhouette Score:", sil_vals)
```

---

> **make\_blobs, Agglomerative Clustering with average linkage, visualize**

```python
X, _ = make_blobs(n_samples=300, centers=3, random_state=42)
agg = AgglomerativeClustering(n_clusters=3, linkage='average').fit(X)
plt.scatter(X[:,0], X[:,1], c=agg.labels_)
plt.show()
```

---

> **Load Wine, apply KMeans, visualize clusters with seaborn pairplot (first 4 features)**

```python
wine = load_wine()
kmeans = KMeans(n_clusters=3).fit(wine.data)
import pandas as pd
df = pd.DataFrame(wine.data[:, :4], columns=wine.feature_names[:4])
df['cluster'] = kmeans.labels_
sns.pairplot(df, hue='cluster')
plt.show()
```

---

> **Noisy blobs with make\_blobs, use DBSCAN to identify clusters and noise, print counts**

```python
X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)
db = DBSCAN(eps=1.2, min_samples=5).fit(X)
labels = db.labels_
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)
print(f"Clusters: {n_clusters}, Noise points: {n_noise}")
```

---

> **Load Digits, reduce with t-SNE, Agglomerative Clustering, plot clusters**

```python
digits = load_digits()
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(digits.data)
agg = AgglomerativeClustering(n_clusters=10).fit(X_tsne)
plt.scatter(X_tsne[:,0], X_tsne[:,1], c=agg.labels_, cmap='tab10')
plt.show()
```
"""