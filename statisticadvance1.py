# -*- coding: utf-8 -*-
"""StatisticAdvance1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l64nZ0VTJ7hXdocUWObmefeqCOcJISMr
"""



"""### 1. What is a random variable in probability theory?

A **random variable** is a function that assigns a numerical value to each possible outcome of a random experiment. It quantifies outcomes of a probabilistic process.

---

### 2. Types of random variables:

* **Discrete random variable:** Takes on countable, distinct values (e.g., number of heads in coin tosses).
* **Continuous random variable:** Takes on any value in a continuous range (e.g., height, weight).

---

### 3. Difference between discrete and continuous distributions:

* **Discrete distribution:** Probability is assigned to specific isolated points. Probability mass function (PMF) describes it.
* **Continuous distribution:** Probability is assigned over intervals; exact values have zero probability. Probability density function (PDF) describes it.

---

### 4. What are probability distribution functions (PDF)?

A **Probability Distribution Function** (more precisely Probability Density Function for continuous variables) describes the likelihood of a random variable taking on a particular value (continuous) or discrete values (PMF).

---

### 5. How do cumulative distribution functions (CDF) differ from PDFs?

* **PDF** (for continuous) or PMF (for discrete) gives the probability density or exact probabilities at specific values.
* **CDF** gives the probability that the random variable is less than or equal to a certain value. It is the integral (continuous) or sum (discrete) of the PDF/PMF up to that point.

---

### 6. What is a discrete uniform distribution?

A discrete uniform distribution assigns equal probability to all outcomes in a finite set (e.g., rolling a fair die where each face has 1/6 probability).

---

### 7. Key properties of a Bernoulli distribution:

* Represents two outcomes: success (1) with probability *p* and failure (0) with probability *(1-p)*.
* Mean = *p*, Variance = *p(1-p)*.
* Models a single trial of a binary event.

---

### 8. What is the binomial distribution, and how is it used?

The binomial distribution models the number of successes in *n* independent Bernoulli trials, each with success probability *p*. Used in scenarios like counting heads in coin tosses.

---

### 9. What is the Poisson distribution and where is it applied?

The Poisson distribution models the count of events occurring in a fixed interval of time or space when these events happen independently and at a constant average rate (e.g., number of calls at a call center per hour).

---

### 10. What is a continuous uniform distribution?

A continuous uniform distribution assigns equal probability density over an interval \[a, b]. The PDF is constant between a and b.

---

### 11. Characteristics of a normal distribution:

* Symmetrical, bell-shaped curve.
* Defined by mean (μ) and variance (σ²).
* About 68% of data within 1σ, 95% within 2σ, 99.7% within 3σ.
* Many natural phenomena approximate this distribution.

---

### 12. What is the standard normal distribution and why is it important?

* A normal distribution with mean 0 and variance 1.
* Used as a reference to standardize other normal distributions via **Z-scores**.
* Enables use of standard tables for probabilities.

---

### 13. What is the Central Limit Theorem (CLT) and why is it critical?

CLT states that the sum or average of a large number of independent, identically distributed random variables tends toward a normal distribution, regardless of the original distribution. This justifies the normal approximation in many statistical methods.

---

### 14. How does CLT relate to the normal distribution?

CLT explains why the normal distribution arises frequently in practice—it emerges as the distribution of sample means for large samples.

---

### 15. Application of Z statistics in hypothesis testing:

Z-tests compare sample means to a known population mean when variance is known or sample size is large, using Z-scores to calculate p-values and decide on rejecting the null hypothesis.

---

### 16. How do you calculate a Z-score and what does it represent?

**Z-score = (X - μ) / σ**
It measures how many standard deviations an observation X is from the mean μ. It standardizes values for comparison.

---

### 17. Point estimates and interval estimates in statistics:

* **Point estimate:** Single value estimate of a population parameter (e.g., sample mean).
* **Interval estimate:** Range of values (confidence interval) likely to contain the parameter.

---

### 18. Significance of confidence intervals in statistical analysis:

Confidence intervals provide a range of plausible values for the parameter, reflecting estimation uncertainty.

---

### 19. Relationship between Z-score and confidence interval:

Confidence intervals are constructed using Z-scores corresponding to desired confidence levels (e.g., 1.96 for 95% CI) to determine the margin of error.

---

### 20. How are Z-scores used to compare different distributions?

Z-scores normalize data from different distributions to a standard scale, enabling comparison regardless of original units or scales.

---

### 21. Assumptions for applying the Central Limit Theorem:

* Samples are independent.
* Sample size is sufficiently large (often n > 30).
* Variables have finite variance.
* Identically distributed variables are ideal but not strictly necessary.

---

### 22. Concept of expected value in a probability distribution:

Expected value (mean) is the long-run average or weighted average outcome of a random variable based on its probabilities.

---

### 23. How does a probability distribution relate to the expected outcome of a random variable?

The probability distribution defines probabilities for outcomes, and the expected value is calculated by summing or integrating outcomes weighted by these probabilities, representing the average predicted value.

---

### 1. Generate a random variable and display its value

```python
import numpy as np

# Generate a random variable from uniform [0,1)
random_var = np.random.rand()
print(f"Random variable value: {random_var}")
```

---

### 2. Generate a discrete uniform distribution and plot the PMF

```python
import matplotlib.pyplot as plt
from scipy.stats import randint

# Define discrete uniform distribution over integers [1, 6]
a, b = 1, 6
dist = randint(a, b+1)  # randint is inclusive of low, exclusive of high

x = np.arange(a, b+1)
pmf = dist.pmf(x)

plt.stem(x, pmf, use_line_collection=True)
plt.title('Discrete Uniform Distribution PMF')
plt.xlabel('Value')
plt.ylabel('Probability')
plt.show()
```

---

### 3. Function to calculate PDF of Bernoulli distribution

```python
def bernoulli_pmf(x, p):
    if x not in (0, 1):
        return 0
    return p**x * (1-p)**(1-x)

# Example usage
p = 0.7
print(f"P(X=1): {bernoulli_pmf(1, p)}")
print(f"P(X=0): {bernoulli_pmf(0, p)}")
```

---

### 4. Simulate binomial distribution with n=10, p=0.5, and plot histogram

```python
n, p = 10, 0.5
binomial_samples = np.random.binomial(n, p, size=1000)

plt.hist(binomial_samples, bins=np.arange(n+2)-0.5, density=True, edgecolor='black')
plt.title('Binomial Distribution (n=10, p=0.5)')
plt.xlabel('Number of successes')
plt.ylabel('Frequency')
plt.show()
```

---

### 5. Create Poisson distribution and visualize it

```python
from scipy.stats import poisson

lambda_ = 3
x = np.arange(0, 15)
pmf = poisson.pmf(x, lambda_)

plt.stem(x, pmf, use_line_collection=True)
plt.title('Poisson Distribution PMF (λ=3)')
plt.xlabel('Number of events')
plt.ylabel('Probability')
plt.show()
```

---

### 6. Calculate and plot CDF of discrete uniform distribution

```python
# Using discrete uniform from 1 to 6 again
cdf = dist.cdf(x)

plt.step(x, cdf, where='post')
plt.title('CDF of Discrete Uniform Distribution')
plt.xlabel('Value')
plt.ylabel('Cumulative Probability')
plt.ylim(0,1.1)
plt.show()
```

---

### 7. Generate continuous uniform distribution and visualize

```python
continuous_uniform_samples = np.random.uniform(low=0, high=1, size=1000)

plt.hist(continuous_uniform_samples, bins=30, density=True, edgecolor='black')
plt.title('Continuous Uniform Distribution (0,1)')
plt.xlabel('Value')
plt.ylabel('Density')
plt.show()
```

---

### 8. Simulate data from normal distribution and plot histogram

```python
normal_samples = np.random.normal(loc=0, scale=1, size=1000)

plt.hist(normal_samples, bins=30, density=True, edgecolor='black')
plt.title('Normal Distribution (μ=0, σ=1)')
plt.xlabel('Value')
plt.ylabel('Density')
plt.show()
```

---

### 9. Function to calculate Z-scores and plot

```python
def plot_z_scores(data):
    mean = np.mean(data)
    std = np.std(data)
    z_scores = (data - mean) / std

    plt.scatter(range(len(data)), z_scores)
    plt.axhline(0, color='red', linestyle='--')
    plt.title('Z-scores of the Dataset')
    plt.xlabel('Index')
    plt.ylabel('Z-score')
    plt.show()
    return z_scores

# Example with normal samples
z_scores = plot_z_scores(normal_samples)
```

---

### 10. Implement Central Limit Theorem (CLT) for a non-normal distribution

```python
# Non-normal distribution: Exponential
sample_size = 30
num_samples = 1000

# Generate many sample means from exponential distribution
means = [np.mean(np.random.exponential(scale=1, size=sample_size)) for _ in range(num_samples)]

plt.hist(means, bins=30, density=True, edgecolor='black')
plt.title(f'CLT Demonstration: Distribution of Sample Means (n={sample_size})')
plt.xlabel('Sample mean')
plt.ylabel('Density')
plt.show()
```

### . Simulate multiple samples from a normal distribution and verify CLT

```python
import numpy as np
import matplotlib.pyplot as plt

def verify_clt(sample_size=30, num_samples=1000):
    # Generate many sample means from normal distribution
    means = [np.mean(np.random.normal(loc=5, scale=2, size=sample_size)) for _ in range(num_samples)]
    
    plt.hist(means, bins=30, density=True, edgecolor='black')
    plt.title(f'CLT Verification: Sample Means Distribution (n={sample_size})')
    plt.xlabel('Sample mean')
    plt.ylabel('Density')
    plt.show()

verify_clt()
```

---

### . Function to calculate and plot standard normal distribution (mean=0, std=1)

```python
from scipy.stats import norm

def plot_standard_normal():
    x = np.linspace(-4, 4, 1000)
    pdf = norm.pdf(x, 0, 1)
    
    plt.plot(x, pdf, label='Standard Normal PDF')
    plt.title('Standard Normal Distribution (mean=0, std=1)')
    plt.xlabel('x')
    plt.ylabel('Probability Density')
    plt.legend()
    plt.show()

plot_standard_normal()
```

---

### . Generate random variables and calculate probabilities (Binomial distribution)

```python
from scipy.stats import binom

n, p = 10, 0.5
k_values = np.arange(0, n+1)

# Generate random variables (samples)
samples = np.random.binomial(n, p, size=1000)

# Calculate binomial probabilities for each k
probabilities = binom.pmf(k_values, n, p)

print("k values:", k_values)
print("Binomial probabilities:", probabilities)
```

---

### . Calculate Z-score for a data point and compare with standard normal distribution

```python
def z_score(x, mean, std):
    return (x - mean) / std

data_point = 70
sample_mean = 65
sample_std = 8

z = z_score(data_point, sample_mean, sample_std)
print(f"Z-score of data point {data_point} is {z}")

# Compare to standard normal CDF
from scipy.stats import norm
p_value = norm.cdf(z)
print(f"CDF at Z = {z}: {p_value}")
```

---

### . Hypothesis testing using Z-statistics

```python
def z_test(sample_mean, pop_mean, pop_std, n):
    z_stat = (sample_mean - pop_mean) / (pop_std / np.sqrt(n))
    p_value = 2 * (1 - norm.cdf(abs(z_stat)))  # two-tailed test
    return z_stat, p_value

# Example
sample_mean = 102
pop_mean = 100
pop_std = 15
n = 36

z_stat, p_val = z_test(sample_mean, pop_mean, pop_std, n)
print(f"Z-statistic: {z_stat:.3f}, p-value: {p_val:.4f}")
```

---

### . Create a confidence interval for a dataset and interpret it

```python
def confidence_interval(data, confidence=0.95):
    mean = np.mean(data)
    std_err = np.std(data, ddof=1) / np.sqrt(len(data))
    z_crit = norm.ppf((1 + confidence) / 2)
    margin = z_crit * std_err
    return mean - margin, mean + margin

data = np.random.normal(50, 10, size=100)
ci_low, ci_high = confidence_interval(data)
print(f"{95}% confidence interval: [{ci_low:.2f}, {ci_high:.2f}]")
```

---

### . Generate normal data and calculate confidence interval for mean

```python
data = np.random.normal(loc=100, scale=15, size=50)
ci_low, ci_high = confidence_interval(data)
print(f"95% CI for mean: [{ci_low:.2f}, {ci_high:.2f}]")
```

---

### . Calculate and visualize PDF of normal distribution

```python
x = np.linspace(-4, 4, 1000)
pdf = norm.pdf(x, loc=0, scale=1)

plt.plot(x, pdf)
plt.title('Normal Distribution PDF (mean=0, std=1)')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.show()
```

---

### . Calculate and interpret CDF of Poisson distribution

```python
lambda_ = 4
x = np.arange(0, 15)
cdf = poisson.cdf(x, lambda_)

plt.step(x, cdf, where='post')
plt.title('Poisson Distribution CDF (λ=4)')
plt.xlabel('k')
plt.ylabel('Cumulative Probability')
plt.show()
```

---

### . Simulate random variable from continuous uniform distribution and calculate expected value

```python
samples = np.random.uniform(low=10, high=20, size=1000)
expected_value = np.mean(samples)
print(f"Expected value (mean) of samples: {expected_value}")
```

---

### . Compare standard deviations of two datasets and visualize

```python
data1 = np.random.normal(0, 1, 1000)
data2 = np.random.normal(0, 3, 1000)

std1, std2 = np.std(data1), np.std(data2)
print(f"Std dev of data1: {std1:.2f}, data2: {std2:.2f}")

plt.boxplot([data1, data2], labels=['Data1', 'Data2'])
plt.title('Standard Deviation Comparison')
plt.show()
```

---

### . Calculate range and interquartile range (IQR) of a normal dataset

```python
data = np.random.normal(50, 10, 1000)
range_ = np.ptp(data)  # max - min
iqr = np.percentile(data, 75) - np.percentile(data, 25)
print(f"Range: {range_:.2f}, IQR: {iqr:.2f}")
```

---

### . Implement Z-score normalization and visualize transformation

```python
def z_score_normalization(data):
    return (data - np.mean(data)) / np.std(data)

data = np.random.normal(50, 15, 1000)
normalized_data = z_score_normalization(data)

plt.subplot(1, 2, 1)
plt.hist(data, bins=30, edgecolor='black')
plt.title('Original Data')

plt.subplot(1, 2, 2)
plt.hist(normalized_data, bins=30, edgecolor='black')
plt.title('Z-score Normalized Data')
plt.show()
```

---

### . Calculate skewness and kurtosis of a normal dataset

```python
from scipy.stats import skew, kurtosis

data = np.random.normal(0, 1, 1000)
skewness = skew(data)
kurt = kurtosis(data)

print(f"Skewness: {skewness:.4f}")
print(f"Kurtosis: {kurt:.4f}")
```
"""