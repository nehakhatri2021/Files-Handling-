# -*- coding: utf-8 -*-
"""Ensemble.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QA-tHm5WHExL2_MckIvJDNRofPISAppx

### üéØ **Bagging & Ensemble Basics**

** Can we use Bagging for regression problems?**
Yes, Bagging can be used for regression using models like `BaggingRegressor`, which averages the predictions from multiple base regressors.

**(2 What is the difference between multiple model training and single model training?**
Single model training uses one algorithm, while multiple model training (ensemble) combines several models to improve performance and reduce overfitting.

**.2 Explain the concept of feature randomness in Random Forest.**
Random Forest selects a random subset of features at each split in a tree, introducing diversity and reducing correlation between trees.

* What is OOB (Out-of-Bag) Score?**
OOB score is a validation technique in Bagging where each model is evaluated using data not included in its bootstrap sample, offering an unbiased performance estimate.

---

### üß† **Model Behavior & Evaluation**

\*\*	2 How can you measure the importance of features in a Random Forest model?\*\*
By checking how much each feature reduces impurity (Gini/entropy) or using permutation importance after training.

* Explain the working principle of a Bagging Classifier.**
It trains multiple classifiers on different random subsets (with replacement) and aggregates their predictions via majority vote.

** How do you evaluate a Bagging Classifier‚Äôs performance?**
Using accuracy, precision, recall, F1-score, ROC-AUC, or OOB score depending on the problem type.

\*\*
2 How does a Bagging Regressor work?\*\*
It trains several regressors on random data subsets and averages their predictions for final output.

---

### üåç **Ensemble Techniques**

** What is the main advantage of ensemble techniques?**
Improved accuracy, robustness, and generalization by combining multiple weak or strong learners.

** What is the main challenge of ensemble methods?**
Increased complexity, longer training times, and reduced interpretability.

** Explain the key idea behind ensemble techniques.**
Combining predictions from multiple models to reduce variance, bias, or improve performance.

** What are the main types of ensemble techniques?**

1. Bagging
2. Boosting
3. Stacking
4. Voting

** What is ensemble learning in machine learning?**
It‚Äôs the process of combining multiple models to achieve better performance than any individual model.

** When should we avoid using ensemble methods?**
When data is small, models are already performing well, or interpretability is crucial.

---

### üå≤ **Random Forest Specifics**

** How does Bagging help in reducing overfitting?**
By averaging predictions from multiple models trained on diverse data samples, Bagging reduces variance.

** Why is Random Forest better than a single Decision Tree?**
It reduces overfitting, increases accuracy, and generalizes better due to ensemble voting/averaging.

** What is the role of bootstrap sampling in Bagging?**
It creates varied training sets for each model, promoting diversity and reducing overfitting.

** What are some real-world applications of ensemble techniques?**

* Credit scoring
* Disease diagnosis
* Spam filtering
* Fraud detection
* Stock market prediction

** What is the difference between Bagging and Boosting?**

* **Bagging:** Trains models independently in parallel, focuses on reducing variance.
* **Boosting:** Trains sequentially, each model corrects the errors of the previous, reducing bias.

from sklearn.datasets import load_breast_cancer, make_regression
from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score
import numpy as np

# Load classification dataset
data = load_breast_cancer()
X_cls, y_cls = data.data, data.target
X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.3, random_state=42)

# Create regression dataset
X_reg, y_reg = make_regression(n_samples=200, n_features=10, noise=0.1, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)

# 1. Bagging Classifier with Decision Tree
bag_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=42)
bag_dt.fit(X_train_cls, y_train_cls)
print("Bagging Classifier Accuracy:", accuracy_score(y_test_cls, bag_dt.predict(X_test_cls)))

# 2. Bagging Regressor with Decision Tree
bag_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), random_state=42)
bag_reg.fit(X_train_reg, y_train_reg)
print("Bagging Regressor MSE:", mean_squared_error(y_test_reg, bag_reg.predict(X_test_reg)))

# 3. Random Forest Classifier
rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train_cls, y_train_cls)
print("Random Forest Classifier Accuracy:", accuracy_score(y_test_cls, rf_clf.predict(X_test_cls)))
print("Top 5 Feature Importances (RF):", rf_clf.feature_importances_[:5])

# 4. Random Forest Regressor vs Decision Tree Regressor
rf_reg = RandomForestRegressor(random_state=42)
dt_reg = DecisionTreeRegressor(random_state=42)
rf_reg.fit(X_train_reg, y_train_reg)
dt_reg.fit(X_train_reg, y_train_reg)
print("Random Forest R2:", rf_reg.score(X_test_reg, y_test_reg))
print("Decision Tree R2:", dt_reg.score(X_test_reg, y_test_reg))

# 5. OOB Score
rf_oob = RandomForestClassifier(oob_score=True, random_state=42)
rf_oob.fit(X_train_cls, y_train_cls)
print("Random Forest OOB Score:", rf_oob.oob_score_)

# 6. Bagging Classifier with SVM
bag_svm = BaggingClassifier(base_estimator=SVC(probability=True), random_state=42)
bag_svm.fit(X_train_cls, y_train_cls)
print("Bagging Classifier with SVM Accuracy:", accuracy_score(y_test_cls, bag_svm.predict(X_test_cls)))

# 7. Random Forest with different n_estimators
for n in [10, 50, 100, 200]:
    model = RandomForestClassifier(n_estimators=n, random_state=42)
    model.fit(X_train_cls, y_train_cls)
    acc = accuracy_score(y_test_cls, model.predict(X_test_cls))
    print(f"Random Forest (n_estimators={n}) Accuracy: {acc}")

# 8. Bagging with Logistic Regression, AUC
bag_lr = BaggingClassifier(base_estimator=LogisticRegression(max_iter=5000), random_state=42)
bag_lr.fit(X_train_cls, y_train_cls)
probs = bag_lr.predict_proba(X_test_cls)[:, 1]
print("Bagging Classifier with Logistic Regression AUC:", roc_auc_score(y_test_cls, probs))
"""